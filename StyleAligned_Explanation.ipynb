{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "540d8642-c203-471c-a66d-0d43aabb0706",
      "metadata": {
        "id": "540d8642-c203-471c-a66d-0d43aabb0706"
      },
      "source": [
        "# StyleAligned: Zero-Shot Style Alignment among a Series of Generated Images via Attention Sharing\n",
        "\n",
        "### **Authors**: ***Borgi Alessio***, ***Danese Francesco***\n",
        "\n",
        "### **Abstract**\n",
        "In this notebook we aim to reproduce and enhance **[StyleAligned](https://arxiv.org/abs/2312.02133)**, a novel technique introduced by **Google Research**, for achieving **Style Consistency** in large-scale Text-to-Image (T2I) generative models. While current T2I models excel in creating visually compelling images from textual descriptions, they often struggle to maintain a consistent style across multiple images. Traditional methods to address this require extensive fine-tuning and manual intervention.\n",
        "\n",
        "**StyleAligned** addresses this challenge by introducing minimal **Attention Sharing** during the **Diffusion Process**, ensuring **Style Alignment among generated images** without the need for optimization or fine-tuning (**Zero-Shoot Inference**). The method operates by leveraging a straightforward inversion operation to apply a reference style across various generated images, maintaining high-quality synthesis and fidelity to the provided text prompts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f82b07b",
      "metadata": {
        "id": "7f82b07b"
      },
      "source": [
        "### 0: SETTINGS & IMPORTS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc07f09c",
      "metadata": {
        "id": "cc07f09c"
      },
      "source": [
        "#### 0.1: CLONE REPOSITORY AND GIT SETUP\n",
        "\n",
        "In the following cell, we setup the code, by cloning the repository, setting up the Git configurations, and providing some other useful commands useful for git.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100b46dd",
      "metadata": {
        "id": "100b46dd"
      },
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/alessioborgi/StyleAlignedDiffModels.git\n",
        "\n",
        "# Change directory to the cloned repository\n",
        "%cd StyleAlignedDiffModels\n",
        "%ls\n",
        "\n",
        "# Set up Git configuration\n",
        "!git config --global user.name \"Alessio Borgi\"\n",
        "!git config --global user.email \"alessioborgi3@gmail.com\"\n",
        "\n",
        "# Stage the changes\n",
        "#!git add .\n",
        "\n",
        "# Commit the changes\n",
        "#!git commit -m \"Added some content to your-file.txt\"\n",
        "\n",
        "# Push the changes (replace 'your-token' with your actual personal access token)\n",
        "#!git push origin main"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f941e8d",
      "metadata": {
        "id": "5f941e8d"
      },
      "source": [
        "#### 0.2: INSTALL AND IMPORT REQUIRED LIBRARIES\n",
        "\n",
        "We proceed then by installing and importing the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94180e84",
      "metadata": {
        "id": "94180e84"
      },
      "outputs": [],
      "source": [
        "# Install the required packages\n",
        "!pip install -r requirements.txt > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23d54ea7-f7ab-4548-9b10-ece87216dc18",
      "metadata": {
        "id": "23d54ea7-f7ab-4548-9b10-ece87216dc18",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import einops\n",
        "import mediapy\n",
        "import inversion\n",
        "import sa_handler\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from typing import Callable\n",
        "from dataclasses import dataclass\n",
        "from __future__ import annotations\n",
        "from diffusers.utils import load_image\n",
        "from torch.nn import functional as nnf\n",
        "from diffusers.models import attention_processor\n",
        "from diffusers import StableDiffusionXLPipeline, DDIMScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f340ee7f",
      "metadata": {
        "id": "f340ee7f"
      },
      "source": [
        "### 1: UTILS IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dd91c40",
      "metadata": {
        "id": "5dd91c40"
      },
      "source": [
        "#### 1.1: ADAIN MODULE\n",
        "\n",
        "The **[Adaptive Instance Normalization (AdaIN)](https://arxiv.org/abs/1703.06868)** module is essential for **StyleAligned**. This works by first computing the mean and standard deviation of the input feature tensor $x$, independently for each feature map. The mean $\\mu_x$ and standard deviation $\\sigma_x$ are calculated as: $$\\mu_x = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_{ij}$$ and $$\\sigma_x = \\sqrt{\\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (x_{ij} - \\mu_x)^2 + \\epsilon}$$, where $H$ and $W$ are the height and width of the feature map, respectively, and $\\epsilon$ is a small constant for numerical stability. These statistics are then matched to those of the style features $y$ by normalizing the input features $x$ and then scaling and shifting them using the style's mean $\\mu_y$ and standard deviation $\\sigma_y$.\n",
        "\n",
        "The transformed feature tensor is given by: $$\\text{AdaIN}(x, y) = \\sigma_y \\left( \\frac{x - \\mu_x}{\\sigma_x} \\right) + \\mu_y$$\n",
        "\n",
        "AdaIN receives a content input $x$ and a style input $y$, and simply aligns the channel wise mean and variance of $x$ to match those of $y$.\n",
        "This process enables the content to adopt the style's statistical properties, facilitating effective style transfer, adding almost no computational cost.\n",
        "\n",
        "In the StyleAligned project, instead of applying this normalization on convolutional-extracted feature maps, we embed it in the self attention layer: the AdaIN module is utilized to normalize the Queries $Q_t$ and Keys $K_t$ of the target image using the Queries $Q_r$ and Keys $K_r$ of the reference image:\n",
        "\n",
        "$$\\hat Q_t = \\text{AdaIN}(Q_t, Q_r) \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\hat K_t = \\text{AdaIN}(K_t, K_r)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ignore\n",
        "'''\n",
        "feat = torch.randn(16, 10, 1, 32)\n",
        "b = feat.shape[0]\n",
        "feat_style = torch.stack((feat[0], feat[b // 2])).unsqueeze(1)\n",
        "print(feat_style.shape)\n",
        "feat_style = feat_style.expand(2, b // 2, *feat.shape[1:])\n",
        "print(feat_style.shape)\n",
        "feat_style.reshape(*feat.shape).shape\n",
        "'''"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NOBQz6J6y3gm",
        "outputId": "dd5d833d-0757-490d-d815-b6a3d9062ac9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NOBQz6J6y3gm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1, 10, 1, 32])\n",
            "torch.Size([2, 8, 10, 1, 32])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 10, 1, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T = torch.tensor # Create Alias for torch.tensor to increase readability\n",
        "\n",
        "def concat_first(feat: T, dim=2) -> T:\n",
        "    feat_style = expand_first(feat)\n",
        "    return torch.cat((feat, feat_style), dim=dim)\n",
        "\n",
        "def expand_first(feat: T) -> T: # this takes mean and std -> input shape: (batch, heads, 1, channels), see below\n",
        "    b = feat.shape[0] # Extract batch size\n",
        "    feat_style = torch.stack((feat[0], feat[b // 2])).unsqueeze(1) # shape: (2, 1, heads, 1, channels), stack the mean (or std) of first and middle images in the batch\n",
        "    feat_style = feat_style.expand(2, b // 2, *feat.shape[1:]) # repeat the mean or std batch/2 times (since we are considering 2 stats)\n",
        "    return feat_style.reshape(*feat.shape) # reshape so that first half of batch has assigned the mean/std of the first img, second half of the middle image\n",
        "\n",
        "def calc_mean_std(feat, eps: float = 1e-5) -> tuple[T, T]:  # computes mean and std along number of tokens dimension\n",
        "    feat_std = (feat.var(dim=-2, keepdims=True) + eps).sqrt()\n",
        "    feat_mean = feat.mean(dim=-2, keepdims=True)\n",
        "    return feat_mean, feat_std # output shape: (batch, heads, 1, channels)\n",
        "\n",
        "def adain(feat: T) -> T: # Input shape: (Batch, Heads, #Tokens, Channels), #Tokens is number of \"pixels\" in the feature map, channels = token_dim\n",
        "    feat_mean, feat_std = calc_mean_std(feat)\n",
        "    feat_style_mean = expand_first(feat_mean)\n",
        "    feat_style_std = expand_first(feat_std)\n",
        "    feat = (feat - feat_mean) / feat_std  # normalize the feature map\n",
        "    feat = feat * feat_style_std + feat_style_mean  # scale and shift the feature map (reparameterization with reference stats)\n",
        "    return feat\n"
      ],
      "metadata": {
        "id": "fcdqDJlVwmPE"
      },
      "id": "fcdqDJlVwmPE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QiWG9J6ahapX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiWG9J6ahapX",
        "outputId": "61cbc271-09b1-4807-dc2a-69ccf24d0e94",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 10, 256, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import torch\n",
        "# create an example feature tensor of shape (Batch, h, t, c)\n",
        "q_test = torch.randn(16, 10, 256, 32)\n",
        "query_after_adain = adain(q_test)\n",
        "query_after_adain.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers.models import attention_processor"
      ],
      "metadata": {
        "id": "e1p6uEYyl07O",
        "outputId": "1969217c-8ef7-422d-ffd4-48f42123b955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "559605b074094805b43e8eac048cf910",
            "06f5d7f994aa43388b7020976a0b65f1",
            "7a12797fa2f94e9692168907384544eb",
            "62de0ccd03344706ab8221f5bd77277c",
            "981afaa972a34bb9ad01028a75c80d36",
            "a1c6deca986341cebc4cd8954081600b",
            "659ff92781d94d2796896cb50aead054",
            "0372abb9ee8e472c97e6413bbf98861c",
            "170a2fd37187478ab8efbe8d7a23cc1e",
            "8db7772e6feb4574be618cadfbfbbb70",
            "f89d8573f0fa4982bf41dd5dcaa33079"
          ]
        }
      },
      "id": "e1p6uEYyl07O",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "559605b074094805b43e8eac048cf910"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn = attention_processor.Attention(query_dim=32, dim_head = 64, heads = 8)\n",
        "# both Q and K and V are projected from the input hidden states to a dimension that matches heads * dim_head.\n",
        "# example: Projected from (Batch, tokens, 32) to (Batch, tokens, 512), and then reshaped to (Batch, tokens, 8 = heads, 64)."
      ],
      "metadata": {
        "id": "tPUwicCtnE1y"
      },
      "id": "tPUwicCtnE1y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn"
      ],
      "metadata": {
        "id": "4YPGMZgWqaXQ",
        "outputId": "da944f1c-2cc1-4c69-a32a-385186df005e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4YPGMZgWqaXQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Attention(\n",
              "  (to_q): Linear(in_features=32, out_features=512, bias=False)\n",
              "  (to_k): Linear(in_features=32, out_features=512, bias=False)\n",
              "  (to_v): Linear(in_features=32, out_features=512, bias=False)\n",
              "  (to_out): ModuleList(\n",
              "    (0): Linear(in_features=512, out_features=32, bias=True)\n",
              "    (1): Dropout(p=0.0, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if h_s comes form previous conv layer, it has the shape (batch, channels, height, width)\n",
        "# therefore we reshape it to have shape (batch, height * width, channels) creating the \"tokens\"\n",
        "# ready to be turned into queries, keys and values and be fed to the self-attention mechanism\n",
        "hidden_states = torch.randn(16, 256, 32) # shape (batch, #tokens = w*h, channels)\n",
        "query = attn.to_q(hidden_states) # to_q, to_k and to_v linearly projects from \"channels\" to \"dim_head * heads\" ...\n",
        "key = attn.to_k(hidden_states) # ... we'll need to reshape them back after to \"divide\" the heads\n",
        "value = attn.to_v(hidden_states)"
      ],
      "metadata": {
        "id": "yD4hFRZG41FP"
      },
      "id": "yD4hFRZG41FP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DefaultAttentionProcessor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.processor = attention_processor.AttnProcessor2_0() # from diffusers.models import attention_processor\n",
        "\n",
        "    def __call__(self, attn: attention_processor.Attention, hidden_states, encoder_hidden_states=None,\n",
        "                 attention_mask=None, **kwargs):\n",
        "        return self.processor(attn, hidden_states, encoder_hidden_states, attention_mask)\n",
        "\n",
        "class SharedAttentionProcessor(DefaultAttentionProcessor):\n",
        "\n",
        "    def shifted_scaled_dot_product_attention(self, attn: attention_processor.Attention, query: T, key: T, value: T) -> T:\n",
        "        logits = torch.einsum('bhqd,bhkd->bhqk', query, key) * attn.scale\n",
        "        logits[:, :, :, query.shape[2]:] += self.shared_score_shift\n",
        "        probs = logits.softmax(-1)\n",
        "        return torch.einsum('bhqk,bhkd->bhqd', probs, value)\n",
        "\n",
        "    def shared_call(\n",
        "            self,\n",
        "            attn: attention_processor.Attention,\n",
        "            hidden_states,\n",
        "            encoder_hidden_states=None,\n",
        "            attention_mask=None,\n",
        "            **kwargs\n",
        "    ):\n",
        "\n",
        "        residual = hidden_states\n",
        "        input_ndim = hidden_states.ndim\n",
        "        if input_ndim == 4:\n",
        "            batch_size, channel, height, width = hidden_states.shape\n",
        "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
        "        batch_size, sequence_length, _ = (\n",
        "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
        "        )\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
        "            # scaled_dot_product_attention expects attention_mask shape to be\n",
        "            # (batch, heads, source_length, target_length)\n",
        "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
        "\n",
        "        if attn.group_norm is not None:\n",
        "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "        query = attn.to_q(hidden_states) # linear layer \"channels\" -> \"heads * dim_heads\"\n",
        "        key = attn.to_k(hidden_states) # same as above\n",
        "        value = attn.to_v(hidden_states) # same as above\n",
        "        inner_dim = key.shape[-1] # get \"heads * dim_heads\" value\n",
        "        head_dim = inner_dim // attn.heads # infer \"dim_head\" by dividing for the number of heads\n",
        "\n",
        "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2) # shape all back to (batch, heads, tokens, dim_head)\n",
        "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2) # same as above\n",
        "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2) # same as above\n",
        "        # if self.step >= self.start_inject:\n",
        "        # Adapative Normalization of Q and K (and possibly V)\n",
        "        if self.adain_queries:\n",
        "            query = adain(query)\n",
        "        if self.adain_keys:\n",
        "            key = adain(key)\n",
        "        if self.adain_values: # usually false\n",
        "            value = adain(value)\n",
        "        # shared attention layer\n",
        "        if self.share_attention:\n",
        "            key = concat_first(key, -2, scale=self.shared_score_scale)\n",
        "            value = concat_first(value, -2)\n",
        "            hidden_states = nnf.scaled_dot_product_attention(\n",
        "                query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
        "            )\n",
        "        else:\n",
        "            hidden_states = nnf.scaled_dot_product_attention(\n",
        "                query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
        "            )\n",
        "        # hidden_states = adain(hidden_states)\n",
        "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
        "        hidden_states = hidden_states.to(query.dtype)\n",
        "\n",
        "        # linear proj\n",
        "        hidden_states = attn.to_out[0](hidden_states)\n",
        "        # dropout\n",
        "        hidden_states = attn.to_out[1](hidden_states)\n",
        "\n",
        "        if input_ndim == 4:\n",
        "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
        "\n",
        "        if attn.residual_connection:\n",
        "            hidden_states = hidden_states + residual\n",
        "\n",
        "        hidden_states = hidden_states / attn.rescale_output_factor\n",
        "        return hidden_states\n",
        "\n",
        "    def __call__(self, attn: attention_processor.Attention, hidden_states, encoder_hidden_states=None,\n",
        "                 attention_mask=None, **kwargs):\n",
        "\n",
        "        hidden_states = self.shared_call(attn, hidden_states, hidden_states, attention_mask, **kwargs)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "    def __init__(self, style_aligned_args: StyleAlignedArgs):\n",
        "        super().__init__()\n",
        "        self.share_attention = style_aligned_args.share_attention\n",
        "        self.adain_queries = style_aligned_args.adain_queries\n",
        "        self.adain_keys = style_aligned_args.adain_keys\n",
        "        self.adain_values = style_aligned_args.adain_values\n",
        "        self.full_attention_share = style_aligned_args.full_attention_share\n",
        "        self.shared_score_scale = style_aligned_args.shared_score_scale\n",
        "        self.shared_score_shift = style_aligned_args.shared_score_shift"
      ],
      "metadata": {
        "id": "WMw349W5D6NJ"
      },
      "id": "WMw349W5D6NJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9f12c1b",
      "metadata": {
        "id": "e9f12c1b"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class StyleAlignedArgs:\n",
        "    share_group_norm: bool = True\n",
        "    \"\"\"\n",
        "    Indicates whether to share group normalization across the model.\n",
        "    \"\"\"\n",
        "\n",
        "    share_layer_norm: bool = True\n",
        "    \"\"\"\n",
        "    Indicates whether to share layer normalization across the model.\n",
        "    \"\"\"\n",
        "\n",
        "    share_attention: bool = True\n",
        "    \"\"\"\n",
        "    Indicates whether to share attention mechanisms across the model.\n",
        "    \"\"\"\n",
        "\n",
        "    adain_queries: bool = True\n",
        "    \"\"\"\n",
        "    Indicates whether to apply AdaIN (Adaptive Instance Normalization) to the queries.\n",
        "    \"\"\"\n",
        "\n",
        "    adain_keys: bool = True\n",
        "    \"\"\"\n",
        "    Indicates whether to apply AdaIN to the keys.\n",
        "    \"\"\"\n",
        "\n",
        "    adain_values: bool = False\n",
        "    \"\"\"\n",
        "    Indicates whether to apply AdaIN to the values.\n",
        "    \"\"\"\n",
        "\n",
        "    only_self_level: float = 0.0\n",
        "    \"\"\"\n",
        "    Level of self-attention to be applied exclusively.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d516363",
      "metadata": {
        "id": "1d516363"
      },
      "source": [
        "### 4: DDIM \\& PIPELINE DEFINITION\n",
        "We then proceed to load the **SDXL (Stable Diffusion XL)** Model and configure the **DDIM (Denoising Diffusion Implicit Models) Scheduler**. We then configure the **Pipeline**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e734c603",
      "metadata": {
        "id": "e734c603"
      },
      "source": [
        "#### 4.1: DDIM SCHEDULER\n",
        "\n",
        "The **DDIM Scheduler** is the component used in diffusion models for generating high-quality samples from noise. It controls the denoising process by defining a schedule for adding and removing noise to and from the data. The scheduler is essential in determining how the model transitions from pure noise to a final, coherent image or other data form.\n",
        "\n",
        "In particular, its parameters are:\n",
        "- **beta_start (float)**: Starting value of beta, the variance of the noise schedule.\n",
        "- **beta_end (float)**: Ending value of beta, the variance of the noise schedule.\n",
        "- **beta_schedule (str)**: The type of schedule for beta. (Possible values: \"linear\", \"scaled_linear\", \"squaredcos_cap_v2\", \"sigmoid\").\n",
        "- **clip_sample (bool)**: If True, the samples are clipped to [-1, 1].\n",
        "- **set_alpha_to_one (bool)**: If True, alpha will be set to 1 at the end of the sampling process.\n",
        "- **num_train_timesteps (int)**: The number of diffusion steps used during training.\n",
        "- **timestep_spacing (str)**: The method to space out timesteps.(Possible values: \"linspace\", \"leading\").\n",
        "- **prediction_type (str)**: The type of prediction model used in the scheduler. (Possible values: \"epsilon\", \"sample\", \"v-prediction\").\n",
        "- **trained_betas (torch.Tensor or None)**: Optional tensor of pre-trained betas to use in the scheduler."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5be67335",
      "metadata": {
        "id": "5be67335"
      },
      "source": [
        "##### 4.1.1: DIFFUSION PROCESS\n",
        "\n",
        "The diffusion process involves adding noise to the data over a series of timesteps, which is described by the forward process:\n",
        "\n",
        "$$ q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\alpha_t} \\mathbf{x}_{t-1}, \\beta_t \\mathbf{I}) $$\n",
        "\n",
        "where:\n",
        "- $\\alpha_t$ and $\\beta_t$ are the scaling and noise variance terms, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd5d7a6a",
      "metadata": {
        "id": "fd5d7a6a"
      },
      "source": [
        "##### 4.1.2: REVERSE PROCESS\n",
        "\n",
        "The reverse process aims to recover the data by denoising it, and is given by:\n",
        "\n",
        "$$ p_{\\theta}(\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_{\\theta}(\\mathbf{x}_t, t), \\sigma_t^2 \\mathbf{I}) $$\n",
        "\n",
        "where:\n",
        "- $\\mu_{\\theta}(\\mathbf{x}_t, t)$ is the predicted mean.\n",
        "- $\\sigma_t$ is the standard deviation of the noise at timestep $t$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d39bf6d4",
      "metadata": {
        "id": "d39bf6d4"
      },
      "source": [
        "##### 4.1.3: BETA SCHEDULE\n",
        "\n",
        "The beta values are scheduled over timesteps from `beta_start` to `beta_end`. The schedule can be:\n",
        "- **Linear**:\n",
        "\n",
        "$$ \\beta_t = \\beta_{\\text{start}} + t \\frac{\\beta_{\\text{end}} - \\beta_{\\text{start}}}{T} $$\n",
        "\n",
        "- **Scaled Linear**:\n",
        "\n",
        "$$ \\beta_t = \\beta_{\\text{start}} + t \\left(\\frac{\\beta_{\\text{end}} - \\beta_{\\text{start}}}{T}\\right)^2 $$\n",
        "\n",
        "- **Sigmoid**:\n",
        "\n",
        "$$ \\beta_t = \\beta_{\\text{start}} + (\\beta_{\\text{end}} - \\beta_{\\text{start}}) \\cdot \\text{sigmoid}(t) $$\n",
        "\n",
        "- **Squared Cosine (squaredcos\\_cap\\_v2)**:\n",
        "\n",
        "$$ \\beta_t = \\beta_{\\text{start}} + 0.5 \\left(1 - \\cos\\left(\\frac{t \\pi}{T}\\right)\\right) (\\beta_{\\text{end}} - \\beta_{\\text{start}}) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6df85710",
      "metadata": {
        "id": "6df85710"
      },
      "source": [
        "##### 4.1.4: INFERENCE WITH DDIM\n",
        "\n",
        "During inference, the denoising process can be described as:\n",
        "\n",
        "$$ \\mathbf{x}_{t-1} = \\sqrt{\\alpha_{t-1}} \\left( \\frac{\\mathbf{x}_t - \\sqrt{1 - \\alpha_t} \\mathbf{\\epsilon}_{\\theta}(\\mathbf{x}_t, t)}{\\sqrt{\\alpha_t}} \\right) + \\sqrt{1 - \\alpha_{t-1} - \\sigma_t^2} \\mathbf{\\epsilon}_{\\theta}(\\mathbf{x}_t, t) $$\n",
        "\n",
        "where:\n",
        "- $\\mathbf{\\epsilon}_{\\theta}(\\mathbf{x}_t, t)$ is the noise predicted by the model.\n",
        "- $\\sigma_t$ is the standard deviation for the timestep $t$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2f6f1e6-445f-47bc-b9db-0301caeb7490",
      "metadata": {
        "id": "c2f6f1e6-445f-47bc-b9db-0301caeb7490",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "scheduler_linear = DDIMScheduler(\n",
        "    beta_start=0.00085,                 # Starting value of beta\n",
        "    beta_end=0.012,                     # Ending value of beta\n",
        "    beta_schedule=\"scaled_linear\",      # Type of schedule for beta values\n",
        "    clip_sample=False,                  # Whether to clip samples to a specified range\n",
        "    set_alpha_to_one=False,             # Whether to set alpha to one at the end of the process\n",
        "\n",
        "    num_train_timesteps=1000,           # Number of diffusion steps used during training\n",
        "    timestep_spacing=\"linspace\",        # Method to space out timesteps\n",
        "    prediction_type=\"epsilon\",          # Type of prediction model used in the scheduler\n",
        "    trained_betas=None                  # Optional pre-trained beta values\n",
        ")\n",
        "\n",
        "scheduler = scheduler_linear"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c3120b5",
      "metadata": {
        "id": "5c3120b5"
      },
      "source": [
        "### 4.2: SDXL PIPELINE DEFINITION\n",
        "\n",
        "We then proceed to **load** the **pre-trained `StableDiffusionXLPipeline` model** with specific configurations to optimize for GPU memory usage and ensure efficient processing. Below is a breakdown of each parameter and its purpose:\n",
        "\n",
        "- **pretrained_model_name_or_path**: The name or path of the pre-trained model to be loaded. In this example, we use `\"stabilityai/stable-diffusion-xl-base-1.0\"`, which is a pre-trained model available in the Stability AI repository.\n",
        "- **torch_dtype**: Specifies the data type for the model's tensors. Here, `torch.float16` is used to enable mixed precision, which helps reduce memory usage and improve computation speed.\n",
        "- **variant**: Indicates the model variant. `\"fp16\"` is used to specify 16-bit floating point precision, aligning with the `torch_dtype` parameter.\n",
        "- **use_safetensors**: Determines whether to use the `safetensors` library for safe tensor loading. Setting this to `True` ensures safer model loading.\n",
        "- **scheduler**: An instance of the scheduler to be used for the diffusion process. In this example, we use a `DDIMScheduler` instance configured for efficient sampling.\n",
        "- **revision**: Specifies the model version to use. The default value is `None`, which means the latest version will be used.\n",
        "- **use_auth_token**: The authentication token used for accessing private models. The default value is `None`, meaning no authentication is required.\n",
        "- **cache_dir**: The directory where the downloaded model will be cached. The default value is `None`, which uses the default cache directory.\n",
        "- **force_download**: Forces the model to be downloaded even if it exists locally. The default value is `False`.\n",
        "- **resume_download**: Resumes a partial download if available. The default value is `False`.\n",
        "- **proxies**: A dictionary of proxy servers to use. The default value is `None`, meaning no proxies are used.\n",
        "- **local_files_only**: Uses only local files if set to `True`. The default value is `False`.\n",
        "- **device_map**: Specifies device placement for model layers. The default value is `None`, which uses the default placement.\n",
        "- **max_memory**: Specifies the maximum memory allowed for each device. The default value is `None`, meaning no specific memory limit is set.\n",
        "\n",
        "Finally, the model is moved to the GPU for faster computations using `.to(\"cuda\")`.\n",
        "\n",
        "The use of mixed precision (`torch_dtype=torch.float16` and `variant=\"fp16\"`) helps in reducing memory usage and improving performance. This configuration is particularly useful when working with large models and limited GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "785645f0",
      "metadata": {
        "id": "785645f0"
      },
      "outputs": [],
      "source": [
        "SDXL_Pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\",  # The model name or path\n",
        "    torch_dtype=torch.float16,            # Data type for the model's tensors\n",
        "    variant=\"fp16\",                       # Model variant for 16-bit floating point precision (Mixed Precision)\n",
        "    use_safetensors=True,                 # Use the safetensors library for safe tensor loading\n",
        "    scheduler=scheduler,                  # Scheduler instance for the diffusion process\n",
        "\n",
        "    revision=None,                        # Model version to use, default is None\n",
        "    use_auth_token=None,                  # Authentication token, None means no authentication\n",
        "    cache_dir=None,                       # Directory to cache the downloaded model, None uses default\n",
        "    force_download=False,                 # Force download even if the model exists locally\n",
        "    resume_download=False,                # Resume a partial download if available\n",
        "    proxies=None,                         # Dictionary of proxy servers to use, None means no proxies\n",
        "    local_files_only=False,               # Use only local files if set to True\n",
        "    device_map=None,                      # Device placement for model layers, None uses default placement\n",
        "    max_memory=None                       # Maximum memory allowed for each device, None means no specific limit\n",
        ").to(\"cuda\")                              # Move the model to the GPU for faster computations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb08ebfe",
      "metadata": {
        "id": "bb08ebfe"
      },
      "outputs": [],
      "source": [
        "handler = sa_handler.Handler(SDXL_Pipeline)\n",
        "sa_args = sa_handler.StyleAlignedArgs(share_group_norm=False,\n",
        "                                      share_layer_norm=False,\n",
        "                                      share_attention=True,\n",
        "                                      adain_queries=True,\n",
        "                                      adain_keys=True,\n",
        "                                      adain_values=False\n",
        "                                     )\n",
        "\n",
        "handler.register(sa_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbdd6159",
      "metadata": {
        "id": "fbdd6159"
      },
      "source": [
        "### 5: RUNNING STYLE-ALIGNED with A SET OF PROMPTS WITHOUT REFERENCE IMAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e77869b",
      "metadata": {
        "id": "7e77869b"
      },
      "source": [
        "TO RUN IF YOU HAVE ENOUGH GPU RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cca9256-0ce0-45c3-9cba-68c7eff1452f",
      "metadata": {
        "id": "5cca9256-0ce0-45c3-9cba-68c7eff1452f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# run StyleAligned\n",
        "\n",
        "sets_of_prompts = [\n",
        "  \"a toy train. macro photo. 3d game asset\",\n",
        "  \"a toy airplane. macro photo. 3d game asset\",\n",
        "  \"a toy bicycle. macro photo. 3d game asset\",\n",
        "  \"a toy car. macro photo. 3d game asset\",\n",
        "  \"a toy boat. macro photo. 3d game asset\",\n",
        "]\n",
        "images = SDXL_Pipeline(sets_of_prompts,).images\n",
        "mediapy.show_images(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc6251b8",
      "metadata": {
        "id": "bc6251b8"
      },
      "source": [
        "TO RUN IF YOU HAVEN'T ENOUGH GPU RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d819ad6d-0c19-411f-ba97-199909f64805",
      "metadata": {
        "id": "d819ad6d-0c19-411f-ba97-199909f64805"
      },
      "outputs": [],
      "source": [
        "# run StyleAligned\n",
        "sets_of_prompts = [\n",
        "  \"a toy train. macro photo. 3d game asset\",\n",
        "  \"a toy airplane. macro photo. 3d game asset\",\n",
        "  \"a toy bicycle. macro photo. 3d game asset\",\n",
        "  \"a toy car. macro photo. 3d game asset\",\n",
        "  \"a toy boat. macro photo. 3d game asset\",\n",
        "]\n",
        "# sets_of_prompts = [\n",
        "#   \"a hot hair balloon, simple wooden statue\",\n",
        "#   \"a friendly robot, simple wooden statue\",\n",
        "#   \"a bull, simple wooden statue\",\n",
        "# ]\n",
        "images = []\n",
        "for prompt in sets_of_prompts:\n",
        "    # Generate image for each prompt individually\n",
        "    image = SDXL_Pipeline([prompt]).images[0]\n",
        "    images.append(image)\n",
        "    # Clear CUDA cache to free memory\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Print Memory summary\n",
        "    # print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "\n",
        "mediapy.show_images(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7c1bd22",
      "metadata": {
        "id": "d7c1bd22"
      },
      "source": [
        "### 6: STYLE-ALIGNED WITH REFERENCE IMAGE\n",
        "\n",
        "Load a reference image and perform the inversion process to extract latent representations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e473def",
      "metadata": {
        "id": "9e473def"
      },
      "source": [
        "#### 6.1: LOADING REFERENCE IMAGE & SETTING PARAMETERS\n",
        "\n",
        "As first thing, we **load a Reference Image** from which you will \"copy the style\". Indeed, we will ask for the newly generated images to stick with the same style of the Reference image, i.e., to be **Style-Aligned**.\n",
        "In particular, we will define 3 **Image Parameters** here:\n",
        "- **reference_style**: This is the reference style describing the reference image.\n",
        "- **reference_prompt**: This is the reference prompt describing the reference image.\n",
        "- **reference_image_path**: This is the path to the reference image.\n",
        "\n",
        "As second step, you will set the parameters relative to the **Diffusion Inversion Process**. This process in a.k.a as **Temperature Scaling**, that aims to inject confidence/randomness in a classification/generation model. In this case, we aim at injecting more styleAlignment or randomness to the image generated.\n",
        "\n",
        "In particular, you will set the parameters relative to:\n",
        "- **num_inference_steps**: The number of inference steps to be performed during the Diffusion Inversion Process.\n",
        "- **guidance_scale**: Here we set the parameter to have **Guidance Scale**, a parameter used in **guided diffusion models** to control the influence of the conditioning signal (e.g., a text prompt) during the image generation process, with the purpose to balance the model’s adherence to the conditioning signal and its natural generative tendencies. In particular:\n",
        "\t- **High Guidance Scale (>1)**: Increases the influence of the conditioning signal, making the generated images more aligned with the prompt. The model is more likely to produce images with features that are explicitly described in the prompt, leading to more detailed and specific outputs. Very high values might cause the model to overfit to the prompt, potentially losing some naturalness or introducing artifacts.\n",
        "\t- **Default Guidance Scale (=1)**: A guidance scale of 1.0 means that the model’s predictions are equally balanced between the conditional and unconditional signals, providing a baseline level of adherence to the prompt.\n",
        "\t- **Low Guidance Scale (<1)**: A lower guidance scale reduces the effect of the guiding input, making the generated images less constrained by the prompt. The model has more freedom to generate diverse and potentially more creative outputs that are not strictly bound to the prompt. Very low values might cause the model to generate images that are too generic and not sufficiently aligned with the prompt.\n",
        "- **style_alignment_score_shift**: This parameter is used to **shift** the **scores logarithmically**. In particular, we will have:\n",
        "\t- **High Values (>1)**: This will **increase** the **StyleAlignment**, making therefore the output image to be closer to the reference.\n",
        "\t- **Low Values (<=1)**: This will **decrease** the **StyleAlignment**, making therefore the output image to be farther to the reference.\n",
        "- **style_alignment_score_scale**: This parameter is used to **scale** the scores or weights within the model. More in detail:\n",
        "\t- **High Values (>1)**: This increases the magnitude of the scores, making the model more confident and therefore not varying so much the generation process.\n",
        "\t- **Standard Value (=1)**: This translates in having no rescaling.\n",
        "\t- **Low Values (<1)**: This coincides to having more generalization (injecting randomness) into the generation process.\n",
        "\n",
        "***Special Configuration for Famous Images***\n",
        "\n",
        "For very famous images, it might be beneficial to suppress the **Shared Attention** to the reference image to avoid overfitting or excessive influence from the reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d034bfe",
      "metadata": {
        "id": "5d034bfe"
      },
      "outputs": [],
      "source": [
        "# Set the source style, prompt and path.\n",
        "reference_style = \"medieval painting\"\n",
        "reference_prompt = f'Man laying in a bed, {reference_style}.'\n",
        "reference_image_path = './imgs/medieval-bed.jpeg'\n",
        "\n",
        "# Setting the number of inference steps in the Diffusion Inversion Process.\n",
        "num_inference_steps = 50\n",
        "\n",
        "# Setting the Guidance Scale for the Diffusion Inversion Process.\n",
        "guidance_scale = 10.0\n",
        "\n",
        "# These are some parameters you can Adjust to Control StyleAlignment to Reference Image.\n",
        "style_alignment_score_shift = 2  # higher value induces higher fidelity, set 0 for no shift\n",
        "style_alignment_score_scale = 1.0  # higher value induces higher, set 1 for no rescale\n",
        "\n",
        "# Famous Paintings\n",
        "# style_alignment_score_shift = 1\n",
        "# style_alignment_score_scale = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c32dc685",
      "metadata": {
        "id": "c32dc685"
      },
      "outputs": [],
      "source": [
        "# Load the reference image and resize it to 1024x1024 pixels.\n",
        "ref_image = np.array(load_image(reference_image_path).resize((1024, 1024)))\n",
        "\n",
        "# Display the output image.\n",
        "mediapy.show_image(ref_image, title=\"Reference Image for Style Alignment\", height=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72621c0e",
      "metadata": {
        "id": "72621c0e"
      },
      "source": [
        "#### 6.2: DIFFUSION INVERSION PROCESS\n",
        "\n",
        "\n",
        "In this section, we will undergo the **Diffusion Inversion Process** in order to **align the style of generated images with a reference style**. This process involves several key steps:\n",
        "\n",
        "\n",
        "1. **Set Prompts**:\n",
        "   - We define a set of prompts for generating images. The first prompt refers to the reference image, while the subsequent prompts are used to generate new images.\n",
        "   - The reference style is appended to each of the subsequent prompts to ensure the generated images adhere to the desired style.\n",
        "\n",
        "2. **Configure Style Alignment Handler**:\n",
        "   - We initialize a handler for the Style Aligned (SA) model and configure it using the `StyleAlignedArgs` parameters. These parameters control various aspects of the style alignment process, such as normalization and attention mechanisms.\n",
        "\n",
        "3. **Run Diffusion Inversion**:\n",
        "   - We execute the DDIM inversion process to map the reference image to its latent representation. This inversion allows us to extract latent features that can be used to guide the generation of new images in the desired style.\n",
        "\n",
        "4. **Generate Images**:\n",
        "   - Using the latent representation obtained from the inversion process, we generate new images based on the defined prompts. We will generate random latent vectors of shape (number_of_prompts, 4, 128, 128) from a normal distribution. We will make use of generator to ensure that the random values are reproducible. In this step, I will have also to ensure that the latent vectors have the same data type as required by the model’s UNet.\n",
        "   After this, we will set the first latent vector to the one extracted from the reference image, ensuring that the first generated image closely adheres to the reference style.\n",
        "   The latent features of the reference image are combined with the prompts to produce images that are stylistically aligned with the reference image.\n",
        "\n",
        "5. **Display Results**:\n",
        "   - Finally, we display the generated images to visualize the effect of the style alignment.\n",
        "\n",
        "This process demonstrates how to leverage the power of diffusion models and inversion techniques to generate images with consistent and coherent styles, guided by a reference image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b030817",
      "metadata": {
        "id": "6b030817"
      },
      "outputs": [],
      "source": [
        "# Set of prompts to generate images for. The first refers to the Reference Image. The other to generate images.\n",
        "prompts = [\n",
        "    reference_prompt,\n",
        "    \"A man working on a laptop\",\n",
        "    \"A man eats pizza\",\n",
        "    \"A woman playig on saxophone\",\n",
        "]\n",
        "\n",
        "# Append the reference style to each of subsequent prompts for generating images with the same Style.\n",
        "for i in range(1, len(prompts)):\n",
        "    prompts[i] = f'{prompts[i]}, {reference_style}.'\n",
        "\n",
        "# Configure the StyleAligned Handler using the StyleAlignedArgs.\n",
        "handler = sa_handler.Handler(SDXL_Pipeline)\n",
        "sa_args = sa_handler.StyleAlignedArgs(\n",
        "    share_group_norm=True,\n",
        "    share_layer_norm=True,\n",
        "    share_attention=True,\n",
        "    adain_queries=True,\n",
        "    adain_keys=True,\n",
        "    adain_values=False,\n",
        "    style_alignment_score_shift=np.log(style_alignment_score_shift),\n",
        "    style_alignment_score_scale=style_alignment_score_scale)\n",
        "handler.register(sa_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ecb36ed",
      "metadata": {
        "id": "5ecb36ed"
      },
      "outputs": [],
      "source": [
        "# Execute the Diffusion Inversion Process to map the reference image to its latent representation.\n",
        "DDIM_inv_result = inversion.ddim_inversion(SDXL_Pipeline, ref_image, reference_prompt, num_inference_steps, 2)\n",
        "\n",
        "# Extract the latent representation from the Diffusion Inversion Result that can be used to guide the generation of new images in the desired style.\n",
        "latent_vector_ref_img, inversion_callback = inversion.make_inversion_callback(DDIM_inv_result, offset=5)\n",
        "\n",
        "# Create a Random Number Generator on the CPU.\n",
        "rand_gen = torch.Generator(device='cpu').manual_seed(10)\n",
        "\n",
        "# Generate the images using the latent representation of the reference image as guidance.\n",
        "latents = torch.randn(len(prompts), 4, 128, 128,            # Random Latent Vectors shape\n",
        "                      device='cpu',                         # Latent Vectors on CPU.\n",
        "                      generator=rand_gen,                   # Random Number Generator.\n",
        "                      dtype=SDXL_Pipeline.unet.dtype,).to('cuda:0') # Data Type of the Latent Vectors (same as required by the model's UNet).\n",
        "\n",
        "# Set the first latent vector to the latent representation of the reference image extracted before.\n",
        "latents[0] = latent_vector_ref_img\n",
        "\n",
        "# Generate the images using the provided prompts and the latent vectors.\n",
        "images_a = SDXL_Pipeline(\n",
        "    prompts,                                 # Prompts to generate images for.\n",
        "    latents=latents,                         # Latent Vectors to guide the generation of images.\n",
        "    callback_on_step_end=inversion_callback, # Callback to update the latent vectors during the generation process.\n",
        "    num_inference_steps=num_inference_steps, # Number of Inference Steps to generate the images.\n",
        "    guidance_scale=10.0).images              # Guidance Scale to control the influence of the latent vectors on the generated images.\n",
        "\n",
        "# Display the generated images.\n",
        "handler.remove()\n",
        "mediapy.show_images(images_a, titles=[p[:-(len(reference_style) + 3)] for p in prompts])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "559605b074094805b43e8eac048cf910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06f5d7f994aa43388b7020976a0b65f1",
              "IPY_MODEL_7a12797fa2f94e9692168907384544eb",
              "IPY_MODEL_62de0ccd03344706ab8221f5bd77277c"
            ],
            "layout": "IPY_MODEL_981afaa972a34bb9ad01028a75c80d36"
          }
        },
        "06f5d7f994aa43388b7020976a0b65f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c6deca986341cebc4cd8954081600b",
            "placeholder": "​",
            "style": "IPY_MODEL_659ff92781d94d2796896cb50aead054",
            "value": ""
          }
        },
        "7a12797fa2f94e9692168907384544eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0372abb9ee8e472c97e6413bbf98861c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_170a2fd37187478ab8efbe8d7a23cc1e",
            "value": 0
          }
        },
        "62de0ccd03344706ab8221f5bd77277c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8db7772e6feb4574be618cadfbfbbb70",
            "placeholder": "​",
            "style": "IPY_MODEL_f89d8573f0fa4982bf41dd5dcaa33079",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "981afaa972a34bb9ad01028a75c80d36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1c6deca986341cebc4cd8954081600b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "659ff92781d94d2796896cb50aead054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0372abb9ee8e472c97e6413bbf98861c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "170a2fd37187478ab8efbe8d7a23cc1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8db7772e6feb4574be618cadfbfbbb70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f89d8573f0fa4982bf41dd5dcaa33079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}